{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed454a46",
   "metadata": {},
   "source": [
    "## Introduction to KNN\n",
    "\n",
    "**K-Nearest Neighbors (KNN)** is a **supervised learning** algorithm commonly used in **classification** and **regression** tasks. KNN is simple yet powerful, operating on the principle of **finding K nearest points** to make decisions.\n",
    "\n",
    "### üîπ How it works:\n",
    "- When a new data point needs to be predicted, the algorithm **calculates the distance** between it and all points in the training dataset.\n",
    "- Based on the **K nearest neighbors**, the algorithm assigns the **most common label** (for classification) or **computes the average value** of its neighbors (for regression).\n",
    "\n",
    "### ‚úÖ Advantages of KNN:\n",
    "- **Easy to implement**: The algorithm is relatively simple to understand and implement.\n",
    "- **Effective for basic recognition tasks**: KNN performs well in **classification** and **pattern recognition** problems.\n",
    "- **Flexible in choosing features/distance metrics**: Users can customize **features** and select a suitable **distance metric** for the data.\n",
    "- **Handles multi-class classification well**: KNN can effectively manage **multi-class problems**.\n",
    "- **Efficient with large training data**: The algorithm performs well when a **sufficiently large dataset** is available.\n",
    "\n",
    "### ‚ö†Ô∏è Disadvantages of KNN:\n",
    "- **Difficulty in choosing the optimal K value**: Selecting the best K can be challenging.\n",
    "- **Changing K can alter classification results**: The outcome of classification or regression may vary significantly based on K's value.\n",
    "- **Performance degrades with high-dimensional data**: For **high-dimensional data**, accuracy declines as the difference between the nearest and farthest neighbors diminishes.\n",
    "- **Sensitive to imbalanced data distribution**: The algorithm may be affected by **uneven class distributions**, leading to biased predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7c4bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e57d64",
   "metadata": {},
   "source": [
    "## First of all, we need to know about Distance Metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8387a8c0",
   "metadata": {},
   "source": [
    "### 1. *Manhattan Distance*\n",
    "$Manhattan Distance=\\sum_{i=1}^n |x_i - y_i|$\n",
    "\n",
    "### 2. *Minkowski Distance*\n",
    "$Minkowski Distance=(\\sum_{i=1}^n|x_i-y_i|^p)^\\frac{1}{p}$\n",
    "\n",
    "### 3. *Euclidean Distance*\n",
    "$Euclidean Distance = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2} $\n",
    "\n",
    "### 4. *Chebyshev Distance (Maximun Norm)*\n",
    "$ Chebyshev Distance =  max_i|x_i-y_i|$\n",
    "\n",
    "### 5. *Cosine Similarity*\n",
    "$cos \\theta = \\frac{\\vec{a} \\vec{b}}{||\\vec{a}|| . ||\\vec{b}||}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a3851b",
   "metadata": {},
   "source": [
    "# Guide to Choosing a Distance Metric for KNN\n",
    "\n",
    "## 1. *Manhattan Distance*\n",
    "**When to use:**\n",
    "- When features do not have a linear relationship.\n",
    "- When data is grid-structured or discrete.\n",
    "- When the data contains many outliers ‚Äî Manhattan is less sensitive than Euclidean.\n",
    "\n",
    "**Examples:**\n",
    "- Robot navigation along rows and columns.\n",
    "- Traffic routing in a city with a grid layout.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. *Minkowski Distance*\n",
    "**When to use:**\n",
    "- When you want to control the influence of distance with the parameter `p`.\n",
    "- It is a generalization of Manhattan (`p = 1`) and Euclidean (`p = 2`).\n",
    "- You can experiment with larger `p` values to emphasize differences across dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. *Euclidean Distance*\n",
    "**When to use:**\n",
    "- When features are continuous and already normalized or scaled evenly.\n",
    "- When data lies in real coordinate space (e.g., physical distance on a map).\n",
    "\n",
    "**Note:**  \n",
    "Euclidean distance is very popular but sensitive to outliers and differences in units among features.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. *Chebyshev Distance*\n",
    "**When to use:**\n",
    "- When you want the distance to be determined by the dimension with the greatest difference.\n",
    "- When you're interested in the maximum deviation among features.\n",
    "\n",
    "**Examples:**\n",
    "- King‚Äôs movement in chess (can move one step in any direction).\n",
    "- Tasks involving evaluation of the largest deviation threshold between features.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. *Cosine Similarity*\n",
    "**When to use:**\n",
    "- When you want to compare the **direction** of vectors rather than their magnitude.\n",
    "- When working with high-dimensional sparse data, like text (TF-IDF, Bag-of-Words).\n",
    "\n",
    "**Examples:**\n",
    "- Document classification, finding similar texts.\n",
    "- Sentiment analysis from customer reviews.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: Suggested Metric Selection\n",
    "\n",
    "| *Distance Metric* | *Suitable Data*                     | *Quick Notes*                           |\n",
    "|-------------------|--------------------------------------|-----------------------------------------|\n",
    "| *Manhattan*        | Discrete data, with outliers         | Robust to noise                         |\n",
    "| *Euclidean*        | Continuous, normalized data          | Popular, sensitive to scale/outliers    |\n",
    "| *Minkowski*        | Flexible, tunable with `p`           | General case, try various `p` values    |\n",
    "| *Chebyshev*        | Compare the largest difference       | Dominated by the largest error          |\n",
    "| *Cosine*           | Text data, high-dimensional          | Compares direction, ignores magnitude   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8ef482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Manhanttan_Distance(x,y): \n",
    "    \"\"\" Calculate the Manhattan distance between two vectors. \n",
    "    Parameters: \n",
    "    x (list or np.array): First vector \n",
    "    y (list or np.array): Second vector \n",
    "    Returns: float: Manhattan distance value \n",
    "    \"\"\" \n",
    "    if len(x) != len(y): \n",
    "        raise ValueError(\"Vectors must be of the same length\") \n",
    "    return sum(abs(x - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2bdc7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Minkowski_Euclidean_distance(x,y,p):\n",
    "    \"\"\"\n",
    "    Calculate the Minkowski distance between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    x (list or np.array): First vector\n",
    "    y (list or np.array): Second vector\n",
    "    p (int or float): norm parameter (1 for Manhattan and Minkowski, 2 for Euclidean)\n",
    "    \n",
    "    Returns:\n",
    "    float: distance between x and y\n",
    "    \"\"\"\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Vectors must be of the same length\")\n",
    "    return (sum(abs(x - y) ** p )) ** (1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af5f2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebyshev_distance(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Chebyshev Distance (Maximum Norm) between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    x (list or np.array): First vector\n",
    "    y (list or np.array): Second vector\n",
    "    \n",
    "    Returns:\n",
    "    float: Chebyshev distance value\n",
    "    \"\"\"\n",
    "    if len(x) != len(y):\n",
    "        raise ValueError(\"Vectors must have the same length\")\n",
    "    return max(abs(x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d09d9bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x,y):\n",
    "    return x@y\n",
    "\n",
    "def vector_norm(v):\n",
    "    return np.sqrt(sum(v**2))\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Calculate the Cosine Similarity between two vectors.\n",
    "\n",
    "    Parameters:\n",
    "    a (list or np.array): First vector\n",
    "    b (list or np.array): Second vector\n",
    "\n",
    "    Returns:\n",
    "    float: Cosine similarity value (ranges from -1 to 1)\n",
    "    \"\"\"\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"Vectors must have the same length\")\n",
    "    return dot_product(a,b) / (vector_norm(a) * vector_norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "551ac548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan Distance: 9\n",
      "Minkowski Distance (p=1): 9.0\n",
      "Minkowski Distance (p=2): 5.196152422706632\n",
      "Chebyshev Distance: 3\n",
      "Cosine Similarity: 0.9915\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test case\n",
    "x_test = np.array([2, 3, 4])\n",
    "y_test = np.array([5, 6, 7])\n",
    "\n",
    "# Testing for different values of p\n",
    "print(\"Manhattan Distance:\", Manhanttan_Distance(x_test, y_test))          # Manhanttan_Distance\n",
    "print(\"Minkowski Distance (p=1):\", Minkowski_Euclidean_distance(x_test, y_test, 1))  # Minkowski Distance \n",
    "print(\"Minkowski Distance (p=2):\", Minkowski_Euclidean_distance(x_test, y_test, 2))  # Euclidean Distance\n",
    "print(\"Chebyshev Distance:\", chebyshev_distance(x_test, y_test))  # chebyshev_distance\n",
    "print(\"Cosine Similarity:\", round(cosine_similarity(x_test, y_test), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187a0ca8",
   "metadata": {},
   "source": [
    "### When to Use Distance Weighting in KNN\n",
    "\n",
    "### ‚úÖ Recommended Use Cases:\n",
    "\n",
    "### 1. Noisy or Imbalanced Data\n",
    "- If data is **overlapping** or contains **noise**, weighted KNN helps **reduce misclassification** by giving more importance to closer points.\n",
    "- **Example**: Classifying messy real-world **customer behavior data**.\n",
    "\n",
    "### 2. Classes or Regression Targets Vary by Region\n",
    "- In some cases, **local patterns** might differ across data regions. Distance weighting ensures predictions focus on the **most relevant neighbors**.\n",
    "- **Example**: Predicting **property prices** in different districts.\n",
    "\n",
    "### 3. Regression Problems\n",
    "- Weighted KNN improves **smoothing**, ensuring predictions are **continuous and accurate** rather than abrupt shifts in output.\n",
    "- **Example**: **Temperature forecasting** based on nearby sensors.\n",
    "\n",
    "### 4. Production-Grade Systems\n",
    "- When deploying models in production, **distance-weighted KNN** with normalization enhances **reliability**, making predictions more **robust**.\n",
    "- **Example**: **Fraud detection** where transaction patterns vary widely.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Unweighted KNN Might Be Enough:\n",
    "\n",
    "### 1. Data is Clean, Small & Well-Separated\n",
    "- If classes are **clearly distinct** and there's **no significant noise**, simple nearest-neighbor classification is **sufficient**.\n",
    "- **Example**: Well-labeled **medical test results** with minimal overlap.\n",
    "\n",
    "### 2. Low Complexity, Fast Execution Needed\n",
    "- Weighted KNN requires **extra computation**, so if **speed is more important** than accuracy, unweighted KNN may be **preferred**.\n",
    "- **Example**: Quick **approximate searches** in large databases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d84a88a",
   "metadata": {},
   "source": [
    "# Build Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9adeb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class CustomKNNClassifier:\n",
    "    def __init__(self, n_neighbors=3, distance_metric='euclidean', weights='uniform', p=None):\n",
    "        allowed_metrics = ['manhattan', 'minkowski', 'euclidean', 'chebyshev', 'cosine']\n",
    "        if distance_metric not in allowed_metrics:\n",
    "            raise ValueError(f\"Unsupported distance_metric. Choose from {allowed_metrics}\")\n",
    "        if weights not in ['uniform', 'distance']:\n",
    "            raise ValueError(\"weights must be 'uniform' or 'distance'\")\n",
    "        if distance_metric != 'minkowski' and p is not None:\n",
    "            print(\"Warning: 'p' is only used with Minkowski distance. It will be ignored.\")\n",
    "        \n",
    "        self.k = n_neighbors\n",
    "        self.metric = distance_metric\n",
    "        self.weights = weights\n",
    "        self.p = p if p is not None else 2 if distance_metric == 'minkowski' else None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "\n",
    "    def _distance(self, x, y):\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "\n",
    "        if self.metric == 'manhattan':\n",
    "            return np.sum(np.abs(x - y))\n",
    "        elif self.metric == 'minkowski':\n",
    "            return np.sum(np.abs(x - y) ** self.p) ** (1 / self.p)\n",
    "        elif self.metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((x - y) ** 2))\n",
    "        elif self.metric == 'chebyshev':\n",
    "            return np.max(np.abs(x - y))\n",
    "        elif self.metric == 'cosine':\n",
    "            dot = np.dot(x, y)\n",
    "            norm = np.linalg.norm(x) * np.linalg.norm(y)\n",
    "            return 1 - dot / norm\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported distance metric: {self.metric}\")\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for test_point in X_test:\n",
    "            distances = np.array([self._distance(test_point, x_train) for x_train in self.X_train])\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_labels = self.y_train[k_indices]\n",
    "            k_distances = distances[k_indices]\n",
    "\n",
    "            if self.weights == 'uniform':\n",
    "                most_common = Counter(k_labels).most_common(1)[0][0]\n",
    "                predictions.append(most_common)\n",
    "            elif self.weights == 'distance':\n",
    "                weights = 1 / (k_distances + 1e-10)\n",
    "                class_weights = {}\n",
    "                for label, weight in zip(k_labels, weights):\n",
    "                    class_weights[label] = class_weights.get(label, 0) + weight\n",
    "                predicted_class = max(class_weights.items(), key=lambda x: x[1])[0]\n",
    "                predictions.append(predicted_class)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "class CustomKNNRegressor(CustomKNNClassifier):\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for test_point in X_test:\n",
    "            distances = np.array([self._distance(test_point, x_train) for x_train in self.X_train])\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_values = self.y_train[k_indices]\n",
    "            k_distances = distances[k_indices]\n",
    "\n",
    "            if self.weights == 'uniform':\n",
    "                prediction = np.mean(k_values)\n",
    "            elif self.weights == 'distance':\n",
    "                weights = 1 / (k_distances + 1e-10)\n",
    "                prediction = np.dot(weights, k_values) / np.sum(weights)\n",
    "\n",
    "            predictions.append(prediction)\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f345470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset: \n",
    "data = [\n",
    "    [25, 40000, 5000, 0, 650],\n",
    "    [35, 60000, 12000, 2, 720],\n",
    "    [45, 80000, 20000, 2, 740],\n",
    "    [22, 25000, 3000, 2, 610],\n",
    "    [33, 120000, 15000, 0, 770],\n",
    "    [50, 30000, 10000, 0, 680],\n",
    "    [28, 95000, 8000, 0, 690],\n",
    "    [40, 62000, 11000, 1, 705],\n",
    "    [60, 100000, 25000, 0, 750],\n",
    "    [48, 220000, 50000, 2, 790]\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Age\", \"Income\", \"LoanAmount\", \"Class\", \"CreditScore\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2af28af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into features and labels\n",
    "X = df[[\"Age\", \"Income\", \"LoanAmount\"]].values\n",
    "y_class = df[\"Class\"].values\n",
    "y_regress = df[\"CreditScore\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "881b41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the KNN classifier\n",
    "X_test = [[30, 50000, 7000]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a99f2500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted credit score: 689.8004665476948\n"
     ]
    }
   ],
   "source": [
    "# Intialize and fit the CustomKNNRegressor use weights = 'distance\n",
    "knn_reg = CustomKNNRegressor(n_neighbors=3, distance_metric='euclidean', weights='distance')\n",
    "knn_reg.fit(X, y_regress)\n",
    "pred_score = knn_reg.predict(X_test)\n",
    "print(\"Predicted credit score:\", pred_score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "244c7790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "# Intialize and fit the CustomKNNClassifier use weights = 'uniform'\n",
    "knn_clf = CustomKNNClassifier(n_neighbors=3, distance_metric='euclidean', weights='uniform')\n",
    "knn_clf.fit(X, y_class)\n",
    "pred_class = knn_clf.predict(X_test)\n",
    "print(\"Predicted class:\", pred_class[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
